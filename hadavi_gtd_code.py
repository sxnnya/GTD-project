# -*- coding: utf-8 -*-
"""Hadavi_GTD_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wbvT35N2ywyUvnA90K_mkpPzBTT5UZr9

# Data Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# Render our plots inline
# %matplotlib inline

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Make the graphs a bit prettier, and bigger
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (15, 7)

df = pd.read_csv('gtd2.csv')

df.head()

df.shape

df.columns

"""# Data Pre-processing/cleaning"""

df.isna().sum()

us_df = df.loc[df["country"] == 217]

us_df.shape

us_df.columns

us_df.isna().sum()

us_df.drop(['eventid','iday','approxdate','extended','country_txt', 'region',
                    'region_txt','latitude','longitude',
            'specificity','vicinity','location','summary','alternative','alternative_txt',
            'multiple','suicide','attacktype2','attacktype2_txt','attacktype3',
            'attacktype3_txt','corp1','target1','natlty1','natlty1_txt','targtype2',
            'targtype2_txt','targsubtype2','targsubtype2_txt','corp2','target2',
            'natlty2','natlty2_txt','targtype3','targtype3_txt','targsubtype3',
            'targsubtype3_txt','corp3','target3','natlty3','natlty3_txt','gsubname',
            'gname2','gsubname2','gname3','gsubname3','motive', 'guncertain1',
            'resolution','country','INT_LOG','INT_IDEO','INT_MISC','INT_ANY',
            'scite1','scite2','scite3','dbsource','related','ransompaidus',
            'weapsubtype4_txt','weapsubtype4','weaptype4_txt','weaptype4',
            'claimmode_txt','claim2','claimmode2','claimmode2_txt','claim3',
            'claimmode3','claimmode3_txt','compclaim',
            'weaptype2','weaptype2_txt','weapsubtype2','weapsubtype2_txt',
            'weaptype3','weaptype3_txt','weapsubtype3','weapsubtype3_txt',
            'guncertain3','divert','ransomamtus','ransompaid','ransomnote',
            'ransomamt','kidhijcountry','ndays','nreleased','guncertain2','nhours',
            'hostkidoutcome_txt','hostkidoutcome','nhostkidus','nhostkid','claimmode',
            'propvalue','propcomment','addnotes','propextent_txt','propextent',
            'nperpcap','claimed','nwoundte','nwoundus','nkillus','ransom','weapdetail',
            'weapsubtype1_txt','weapsubtype1','ishostkid','targsubtype1_txt',
            'nwound','nkill','nkillter','individual','nperps','targtype1_txt',
            'attacktype1_txt','weaptype1_txt','doubtterr'], axis=1, inplace = True)

us_df.shape

us_df.isna().sum().sort_values()

us_df.dropna(inplace = True)
us_df.shape

us_df.columns

us_df.dtypes

"""# Data Understanding/Exploration"""

us_df['attacktype1'].value_counts()

us_df['provstate'].value_counts()

us_df['targtype1'].value_counts()

sns.countplot(x = 'attacktype1', data = us_df)

sns.countplot(x = 'attacktype1', hue = 'targtype1', data = us_df)

visualization = us_df[us_df['attacktype1'] == 3]
sns.countplot(x = 'targtype1', data = visualization)

visualization2 = us_df[us_df['attacktype1'] == 7]
sns.countplot(x = 'targtype1', data = visualization2)

visualization3 = us_df[us_df['attacktype1'] == 2]
sns.countplot(x = 'targtype1', data = visualization3)

"""# Data Splitting & Rebalancing"""

from sklearn.model_selection import train_test_split

project_df = us_df

project_df.drop(['iyear','city','gname','crit1','crit2','crit3', 'property', 'success'], axis=1, inplace = True)

project_df.columns

project_df

project_df = pd.get_dummies(us_df, columns=['provstate'], prefix = ['dummy'])

project_df.shape

terror_train1, terror_test1 = train_test_split(project_df, test_size =0.30, random_state = 7)

print(terror_test1.shape)
print(terror_train1.shape)

# balance the dataset
print(terror_train1['attacktype1'].count())
terror_train1['attacktype1'].value_counts()

to_resample = project_df.loc[project_df['attacktype1'] == 6]
to_resample.info()

our_resample = to_resample.sample(n = 25 , replace = True)

# check the resampled set
our_resample.info()

# add the resampled data to the training set
terror_train1_rebal = pd.concat([terror_train1, our_resample])
terror_train1_rebal.info()

to_resample = project_df.loc[project_df['attacktype1'] == 4]
to_resample.info()
our_resample = to_resample.sample(n = 25, replace = True)

# check the resampled set
our_resample.info()
terror_train1_rebal = pd.concat([terror_train1_rebal, our_resample])
terror_train1_rebal.info()

to_resample = project_df.loc[project_df['attacktype1'] == 9]
to_resample.info()
our_resample = to_resample.sample(n = 25 , replace = True)

# check the resampled set
our_resample.info()
terror_train1_rebal = pd.concat([terror_train1_rebal, our_resample])
terror_train1_rebal.info()

print(terror_train1_rebal['attacktype1'].count())
terror_train1_rebal['attacktype1'].value_counts()

"""# Baseline"""

terror_train1_rebal['attacktype1'].value_counts()

"""Since '3' (bombing/explosion) is the most occurring value for this target attribute we will use that as the prediction for our test set.

Our baseline model would correctly classify 936 test sets and incorrectly classify 1155 (2091 - 936).

Our accuracy is around 44.76%.

# Modeling

## Splitting Data
"""

x = terror_train1_rebal.drop(columns=['attacktype1'])

y = terror_train1_rebal['attacktype1']

x_test = terror_test1.drop(columns=['attacktype1'])

y_test = terror_test1['attacktype1']

"""## Models using training set (Decision Trees & K-Nearest Neighbors)"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

cart01 = DecisionTreeClassifier(max_leaf_nodes=5).fit(x,y)
rf01 = RandomForestClassifier(n_estimators = 10,criterion="gini").fit(x,y)

prediction_cart01 = cart01.predict(x)
prediction_rf01 = rf01.predict(x)

def eval_prediction(pred, actual):
    index = 0
    correct = 0
    for outcome in actual:
        if pred[index] == outcome:
            correct += 1

        index+=1
    return correct

print("CART:", eval_prediction(prediction_cart01, y))
print("Random Forest:", eval_prediction(prediction_rf01, y))

print("CART:", '{0:.2f}'.format((eval_prediction(prediction_cart01, y)/len(x))*100),"%")
print("Random Forest:", '{0:.2f}'.format((eval_prediction(prediction_rf01, y)/len(x))*100),"%")

print(cart01.score(x,y))
print(rf01.score(x,y))

from sklearn.metrics import accuracy_score

from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(10)
clf = clf.fit(x,y)
pred = clf.predict(x)
acc = accuracy_score(pred,y)
print(acc)
print(f'K-Nearest Neighbors: {acc*100}%')

print("K-Nearest Neighbors:", eval_prediction(pred, y))

"""## Models using test set (Decision Trees & K-Nearest Neighbors)"""

cart_predict = cart01.predict(x_test)
rf_predict =  rf01.predict(x_test)

print("CART (test):", eval_prediction(cart_predict, y_test))
print("Random Forest (test):", eval_prediction(rf_predict, y_test))

print("CART:", '{0:.2f}'.format((eval_prediction(cart_predict, y_test)/len(x))*100),"%")
print("Random Forest:", '{0:.2f}'.format((eval_prediction(rf_predict, y_test)/len(x))*100),"%")

pred_test = clf.predict(x_test)
acc_test = accuracy_score(pred_test,y_test)
print(acc_test)
print(f'K-Nearest Neighbors: {acc_test*100}%')

print("K-Nearest Neighbors (test):", eval_prediction(pred_test, y_test))

"""## Cross Validation (Decision Trees)"""

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

k = 10
crossvalidation = KFold(n_splits=k, random_state=1, shuffle=True)

cart_cv_scores = cross_val_score(cart01, x_test, y_test, cv=crossvalidation)

print("Cart cross validation scores with k=10: ", cart_cv_scores)
print("Average score of all folds:",cart_cv_scores.mean())

rf01_cv_scores = cross_val_score(rf01, x, y, cv=crossvalidation)

print("Random Forest cross validation scores with k=10: ", rf01_cv_scores)
print("Average score of all folds:",rf01_cv_scores.mean())

"""# Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
nb_model = MultinomialNB() # default params - MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
nb_model.fit(x, y)

print(nb_model.score(x,y))

print(nb_model.score(x_test,y_test))

y_predicted = nb_model.predict(x_test)
ypred = pd.crosstab(y_test, y_predicted, rownames = ['Actual'], colnames = ['Predicted'])
ypred['Total'] = ypred.sum(axis=1)
ypred.loc['Total'] = ypred.sum()
ypred

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

def plot_confusion_matrix(Y_test, Y_preds):
    conf_mat = confusion_matrix(Y_test, Y_preds)
    #print(conf_mat)
    fig = plt.figure(figsize=(6,6))
    plt.matshow(conf_mat, cmap=plt.cm.Blues, fignum=1)
    plt.yticks(range(9), range(9))
    plt.xticks(range(9), range(9))
    plt.colorbar();
    for i in range(9):
        for j in range(9):
            plt.text(i-0.2,j+0.1, str(conf_mat[j, i]), color='tab:red')

plot_confusion_matrix(y_test, nb_model.predict(x_test))

from sklearn.model_selection import GridSearchCV

params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0]}

multinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=-1, cv=10, verbose=5)
multinomial_nb_grid.fit(x,y)

print('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(x, y))
print('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(x_test, y_test))
print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)
print('Best Parameters : ',multinomial_nb_grid.best_params_)

plot_confusion_matrix(y_test, multinomial_nb_grid.best_estimator_.predict(x_test))

"""# Evaluation Second Look"""

import pandas as pd
import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import ComplementNB

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import log_loss
from sklearn.metrics import mean_squared_error
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Let's try Decision Tree


# Summary of the predictions made by the classifier
print(classification_report(y_test, cart_predict))
print('Confusion Matrix')
print(confusion_matrix(y_test, cart_predict))
# Accuracy score
print('accuracy is',accuracy_score(cart_predict,y_test))

test_predictions_proba = nb_model.predict_proba(x_test)
ll = log_loss(y_test, test_predictions_proba)
print("Log Loss: {:.4}".format(ll))
print("="*30)

# Let's try Random Forest as a second model

# Summary of the predictions made by the classifier
print(classification_report(y_test, rf_predict))
print('Confusion Matrix')
print(confusion_matrix(y_test, rf_predict))
# Accuracy score
print('accuracy is',accuracy_score(rf_predict,y_test))

test_predictions_proba = nb_model.predict_proba(x_test)
ll = log_loss(y_test, test_predictions_proba)
print("Log Loss: {:.4}".format(ll))
print("="*30)

# K-Nearest Neighbors as third model

# Summary of the predictions made by the classifier
print(classification_report(y_test, pred_test))
print('Confusion Matrix')
print(confusion_matrix(y_test, pred_test))
# Accuracy score
print('accuracy is',accuracy_score(pred_test,y_test))

test_predictions_proba = nb_model.predict_proba(x_test)
ll = log_loss(y_test, test_predictions_proba)
print("Log Loss: {:.4}".format(ll))
print("="*30)

feature_importances = cart01.feature_importances_
plt.figure(figsize=(80,9))
sns.barplot(x=x.columns,y=feature_importances,alpha=0.8)
plt.xlabel('Features')
plt.ylabel('% of Decisions')
plt.title('Feature Importances for Decision Tree Model')
plt.show()

feature_importances = rf01.feature_importances_
plt.figure(figsize=(90,9))
sns.barplot(x=x.columns,y=feature_importances,alpha=0.8)
plt.xlabel('Features')
plt.ylabel('% of Decisions')
plt.title('Feature Importances for Random Forest Model')
plt.show()

plot_confusion_matrix(y_test, cart_predict)

plot_confusion_matrix(y_test, rf_predict)

plot_confusion_matrix(y_test, pred_test)